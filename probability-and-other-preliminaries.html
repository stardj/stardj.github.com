<!DOCTYPE html>
<html lang="en">
<head>
          <title>Stardj is coding</title>
        <meta charset="utf-8" />


    <meta name="tags" content="Learning note" />

        <link rel="stylesheet" href="/theme/css/main.css" />
        <meta name="viewport" content="width=device-width, initial-scale=1" />
        <!--[if lte IE 8]><script src="/theme/js/ie/html5shiv.js"></script><![endif]-->
        <!--[if lte IE 8]><link rel="stylesheet" href="/theme/css/ie8.css" /><![endif]-->
</head>

<body>
        <header id="banner" class="body">
                <h1><a href="/">Stardj is coding <strong></strong></a></h1>
        </header><!-- /#banner -->
<!-- Sidebar -->
            <div id="sidebar">

                <!-- Logo -->
                    <h1 id="logo"><a href="/">Stardj is coding</a></h1>
                    <!-- Text -->
                    <section class="box text-style1">
                        <div class="inner">
                            <p>
                                <strong>Ut scelerisque nec sapien ut sollicitudin:</strong><br />
                            </p>
                        </div>
                    </section>
                <!-- Nav -->
                    <nav id="nav">
                        <ul>
                            
                            <li><a href="#">Nam euismod dui</a></li>
                            <li><a href="#">Integer eget eros</a></li>
                            <li><a href="#">Proin vehicula tortor</a></li>
                            <li><a href="#">Vestibulum consectetur tellus</a></li>
                        </ul>
                    </nav>

                <!-- Search -->
                    <!-- <section class="box search">
                        <form method="post" action="#">
                            <input type="text" class="text" name="search" placeholder="Search" />
                        </form>
                    </section> -->

                

                <!-- Recent Posts -->
                    <section class="box recent-posts">
                        <header>
                            <h2>Contact Me</h2>
                        </header>
                        <ul>
                            <li><a class="icon fa-envelope" href="#"> eMail</a></li>
                            <li><a class="icon fa-twitter" href="#"> Twitter</a></li>
                            <li><a class="icon fa-facebook" href="#"> Facebook</a></li>
                            <li><a class="icon fa-github" href="#"> Github</a></li>
                            
                        </ul>
                    </section>



                <!-- Copyright -->
                    <ul id="copyright">
                        <li>Proudly powered by <a href="http://getpelican.com/">Pelican</a>,
                which takes great advantage of <a href="http://python.org">Python</a>.</li>
                        <li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
                        <li>Modified under a <a href="https://creativecommons.org/licenses/by/3.0/">CC:BY</a> license.</li>
                    </ul>

            </div><!-- /#menu -->
<div id="content">
<div class="inner">
        <article class="box post post-excerpt">
                <header> 
                <h2><a href="/probability-and-other-preliminaries.html" rel="bookmark" title="Permalink to Probability and Other Preliminaries">Probability and Other Preliminaries</a></h2> 
                <p></p>
                </header>
                <div class="info">
                    <span class="date" datetime="2017-10-01T15:35:00+02:00"><span class="month">Oct</span> <span class="day">01</span> <span class="month"> 2017</span></span>
                    <ul class="stats">
                                    <li><a href="mailto:?Subject=Probability and Other Preliminaries&Body=I%20saw%20this%20and%20thought%20of%20you!%20 /probability-and-other-preliminaries.html" class="icon fa-envelope">&nbsp;</a></li>
                                    <li><a href="http://reddit.com/submit?url=/probability-and-other-preliminaries.html&title=#Probability and Other Preliminaries"" class="icon fa-reddit">&nbsp;</a></li>
                                    <li><a href="https://twitter.com/share?url=/probability-and-other-preliminaries.html&text=&hashtags=GnotC" class="icon fa-twitter">&nbsp;</a></li>
                                    <li><a href="https://www.facebook.com/sharer.php?u=/probability-and-other-preliminaries.html" class="icon fa-facebook">&nbsp;</a></li>
                                </ul>
                </div><!-- /.post-info -->
                <div class="entry-content"> &nbsp; </div><!-- /.entry-content -->
        </article>
    <h1>Probability and Other Preliminaries</h1>
<ul>
<li>probability distribution</li>
<li>rules of probability</li>
<li>review for linear algebra</li>
</ul>
<h3>Preview</h3>
<ul>
<li>topics this week mainly concerns a brief review for probability and linear algebra</li>
<li>lab class introduces use of Jupyter, Python and Pandas</li>
</ul>
<h3>Probability Distribution</h3>
<p>The <strong>probability distribution function</strong> of a random variable $X$ is
$$
  F(x) = P(X \leq x)
$$
where the notation ${X \leq x}$ consists of all outcomes smaller than or equal to $x$.</p>
<p>The derivative
$$
  p(x) = \frac{dF(x)}{dx}
$$
is called the <strong>probability density function</strong> of $X$ .</p>
<p>Wikipedia:
<a href="https://en.wikipedia.org/wiki/Probability_distribution">Probability distribution</a> /
<a href="https://en.wikipedia.org/wiki/Probability_density_function">Probability density function</a></p>
<h3>Joint Distribution</h3>
<p>The <strong>joint distribution function</strong> of two random variables $X$ and $Y$ is the probability of the joint statistics ${X \leq x, Y \leq y}$, ie,
$$
  F(x, y) = P(X \leq x, Y \leq y)
$$</p>
<p>The derivative
$$
  p(x, y) = \frac{\partial^2 F(x, y)}{\partial x \partial y}
$$
is called the <strong>joint density function</strong> of $X$ and $Y$ .</p>
<ul>
<li>$X$ and $Y$ are <strong>independent</strong> if and only if $p(x,y) = p(x)p(y)$</li>
</ul>
<p>Wikipedia: <a href="https://en.wikipedia.org/wiki/Joint_probability_distribution">Joint probability distribution</a></p>
<h3>Conditional Distribution</h3>
<p>Given two jointly distributed random variables $X$ and $Y$, the <strong>conditional probability distribution</strong> of $Y$ given $X$ is the probability distribution of $Y$ when $X$ is known to be a particular value.</p>
<p>The <strong>conditional density function</strong> of $y$ given the occurrence of the value $x$ is
$$
  p(y|x) = \frac{p(x,y)}{p(x)}
$$</p>
<p>Wikipedia: <a href="https://en.wikipedia.org/wiki/Conditional_probability_distribution">Conditional probability distribution</a></p>
<h3>Gaussian (Normal) Distribution</h3>
<p>The probability density of the <strong>Gaussian distribution</strong> is
$$
  p(x\ |\ \mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{(x-\mu)^2}{2\sigma^2} \right)
$$
where $\mu$ is the <strong>mean</strong> and $\sigma^2$ is the <strong>variance</strong> of the distribution.</p>
<ul>
<li>very common in natural and social sciences</li>
<li>$\sigma$ is the <strong>standard deviation</strong></li>
</ul>
<p>Wikipedia: <a href="https://en.wikipedia.org/wiki/Normal_distribution">Normal distribution</a></p>
<h3>The Normal (Gaussian) Distribution</h3>
<div align="right">
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Gaussian_distribution.svg/640px-Gaussian_distribution.svg.png", width=700>
</div>

<ul>
<li>about 68% of values drawn from a Gaussian distribution are within one standard deviation $\sigma$ from the mean $\mu$</li>
</ul>
<h3>Multivariate Gaussian (Normal) Distribution</h3>
<p>The probability density of the $k$-dimensional <strong>Gaussian distribution</strong> is
$$
  p(\mathbf{x}\ |\ \boldsymbol{\mu},\boldsymbol{\Sigma}) = \frac{1}{\sqrt{2\pi^k |\boldsymbol{\Sigma}|}} \exp\left( -\frac{1}{2} (\mathbf{x}-\boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1} (\mathbf{x}-\boldsymbol{\mu}) \right)
$$
where $\boldsymbol{\mu}$ is the $k\times 1$ <strong>mean vector</strong> and $\boldsymbol{\Sigma}$ is the $k\times k$ <strong>covariance matrix</strong>.</p>
<ul>
<li>$|\boldsymbol{\Sigma}|$ and $\boldsymbol{\Sigma}^{-1}$ are the <strong>determinant</strong> and the <strong>inverse</strong> of the covariance</li>
<li>a symbol $~^\top$ indicates the <strong>transpose</strong></li>
</ul>
<p>Wikipedia: <a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution">Multivariate normal distribution</a></p>
<h3>Notation</h3>
<p>Formally we should write out $p(X=x,Y=y)$ .</p>
<p>In practice we often use $p(x,y)$ .</p>
<ul>
<li>this looks very much like we might write a multivariate function, eg, $f(x,y) = \frac{x}{y}$</li>
<li>for a multivariate function though, $f(x,y)\neq f(y,x)$</li>
<li>however $p(x,y) = p(y,x)$ because $p(X=x,Y=y) = p(Y=y,X=x)$</li>
</ul>
<p>We now quickly review the <strong>rules of probability</strong>.</p>
<h3>Normalisation</h3>
<p>All distributions are normalised.</p>
<ul>
<li>this is clear from the fact that $\sum_{x\in {\cal X}} n_{x} = N$, which gives
$$
  \sum_{x\in {\cal X}} p(x) = \lim_{N\rightarrow\infty} \frac{\sum_{x\in {\cal X}} n_x}{N} = \lim_{N\rightarrow\infty} \frac{N}{N} = 1
$$</li>
</ul>
<p>A similar result can be derived for the marginal and conditional distributions.</p>
<h3>Product Rule and Sum Rule</h3>
<p>The product rule of probability:
$$
  \underbrace{p(x,y)}<em _text_conditional="\text{conditional" probability>{\text{joint probability}} = \underbrace{p(y|x)}</em>}\cdot\ p(x)
$$</p>
<p>The sum rule of probability:
$$
  \underbrace{p(y)}<em X _cal="{\cal" x_in="x\in">{\text{marginal probability}} = \sum</em>} p(x,y) = \sum_{x\in {\cal X}} p(y|x)p(x)
$$</p>
<p>Wikipedia:
<a href="https://en.wikipedia.org/wiki/Product_rule">Product rule</a> /
<a href="https://en.wikipedia.org/wiki/Probability_axioms">Probability axioms</a></p>
<h3>Bayes' Theorem</h3>
<p>Bayes' theorem immediately follows the product rule:
$$
  p(x|y) = \frac{p(x,y)}{p(y)} = \frac{p(y|x)p(x)}{\displaystyle \sum_{x\in {\cal X}} p(y|x)p(x)}
$$</p>
<p>Wikipedia: <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes' theorem</a></p>
<p>(<strong>Example</strong>)
There are two barrels in front of you.
Barrel One(B1) contains 20 apples and 4 oranges.
Barrel Two(B2) contains 4 apples and 8 oranges.
You choose a barrel randomly and select a fruit.
It is an <strong>apple</strong>.
What is the probability that the barrel was Barrel One?</p>
<p>(<strong>Solution</strong>) we are given that:
$$
\begin{align<em>}
  p(\text{apple}\ |\ \text{B}_1) = &amp; \frac{20}{24} &amp; \qquad p(\text{B}_1) = 0.5 \
  p(\text{apple}\ |\ \text{B}_2) = &amp; \frac{4}{12} &amp; \qquad p(\text{B}_2) = 0.5
\end{align</em>}
$$
Use the sum rule to calculate
$$
  p(\text{apple}) = p(\text{apple}\ |\ \text{B}_1)p(\text{B}_1) + p(\text{apple}\ |\ \text{B}_2)P(\text{B}_2) = \frac{20}{24}\times 0.5 + \frac{4}{12}\times 0.5 = \frac{7}{12}
$$
and Bayes' theorem tells us that:
$$
  p(\text{B}_1\ |\ \text{apple}) = \frac{p(\text{apple}\ |\ \text{B}_1)P(\text{B}_1)}{P(\text{apple})} = \frac{\frac{20}{24}\times 0.5}{\frac{7}{12}} = \frac{5}{7}
$$</p>
<h3>Expected Value</h3>
<p>The <strong>expected value</strong> (or mean, average) of a random variable $X$ is
$$
  \mathbb{E}[X] = \int_{-\infty}^{\infty} xp(x) dx
$$</p>
<ul>
<li>discrete type is $\mathbb{E}[X] = \sum_{x\in {\cal X}} x p(x)$ for all possible events ${\cal X}$</li>
</ul>
<p>The expected value of a function $f(x)$ is
$$
  \mathbb{E}[f(x)] = \int_{-\infty}^{\infty} f(x) p(x) dx
$$</p>
<p>Wikipedia: <a href="https://en.wikipedia.org/wiki/Expected_value">Expected value</a></p>
<h3>Variance</h3>
<p>The <strong>variance</strong> is the expected value of $f(x) = (x - \mathbb{E}[X])^2$, ie,
$$
  \mathbb{V}ar[X] = \mathbb{E}[(X - \mathbb{E}[X])^2] = \int_{-\infty}^{\infty} (x - \mathbb{E}[X])^2 p(x) dx
$$</p>
<ul>
<li>discrete type is $\mathbb{V}ar[X] = \sum_{x\in {\cal X}} (x - \mathbb{E}[X])^2 p(x)$</li>
</ul>
<p>(note)
$
  \mathbb{V}ar[X]
  = \mathbb{E}[(X - \mathbb{E}[X])^2]
  = \mathbb{E}[X^2 - 2X\mathbb{E}(X) + \mathbb{E}[X]^2]
  = \mathbb{E}[X^2] - 2\mathbb{E}(X)\mathbb{E}(X) + \mathbb{E}[X]^2
  = \mathbb{E}[X^2] - \mathbb{E}[X]^2
$</p>
<p>Wikipedia: <a href="https://en.wikipedia.org/wiki/Variance">Variance</a></p>
<h3>Derivatives with Vectors</h3>
<p>We have scalars $x$, $y$, and $n$- and $m$-dimensional vectors $\mathbf{x}$, $\mathbf{y}$, where
$$
  \mathbf{x} = \left( \begin{array}{c} x_1 \ \vdots \ x_n \end{array} \right)
  \qquad
  \mathbf{y} = \left( \begin{array}{c} y_1 \ \vdots \ y_m \end{array} \right)
$$
Derivatives with vectors using the <strong>denominator-layout</strong> notation:
$$
  \frac{\partial \mathbf{y}}{\partial x} = \left( \frac{\partial y_1}{\partial x} \cdots \frac{\partial y_m}{\partial x} \right)
  \qquad
  \frac{\partial y}{\partial \mathbf{x}} = \left( \begin{array}{c} \frac{\partial y}{\partial x_1} \ \vdots \ \frac{\partial y}{\partial x_n} \end{array} \right)
  \qquad
  \frac{\partial \mathbf{y}}{\partial \mathbf{x}} = \left( \begin{array}{ccc} \frac{\partial y_1}{\partial x_1} &amp; \cdots &amp; \frac{\partial y_m}{\partial x_1} \ \vdots &amp; &amp; \vdots \ \frac{\partial y_1}{\partial x_n} &amp; \cdots &amp; \frac{\partial y_m}{\partial x_n} \end{array} \right)
$$</p>
<h3>Some Scalar-by-Vector Identities</h3>
<p>For vectors $\mathbf{a}$, $\mathbf{w}$ and a square matrix $\mathbf{A}$ :
$$
\begin{align<em>}
  \frac{\partial \mathbf{a}^\top \mathbf{w}}{\partial \mathbf{w}} = &amp; \mathbf{a} \
  \frac{\partial \mathbf{w}^\top \mathbf{A} \mathbf{w}}{\partial \mathbf{w}} = &amp; (\mathbf{A} + \mathbf{A}^\top)\mathbf{w}
\end{align</em>}
$$
Wikipedia: <a href="https://en.wikipedia.org/wiki/Matrix_calculus">Matrix calculus</a></p>
  </div><!-- /.entry-content -->
  <!-- Pagination -->
            <div class="pagination">
            </div>
 
</div>
</div>
<!-- /#contentinfo -->
</body>
</html>